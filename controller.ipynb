{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b0fe692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import libsumo as traci\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import random, numpy as np\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "import csv, os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "SEED = 8\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# Imports and configuration\n",
    "# Use sumo or sumo-gui depending on your preference\n",
    "SUMO_GUI_BINARY  = \"sumo-gui\"  # or \"sumo-gui\"\n",
    "SUMO_BINARY  = \"sumo\"  # or \"sumo-gui\"\n",
    "SUMO_CFG  = \"./sumo_cfg/simulation.sumocfg\"  # your .sumocfg file\n",
    "SUMO_STATE = \"./sumo_cfg/simulation_state.xml\"  # your initial state file\n",
    "SUMO_FIRST_STATE = \"./sumo_cfg/simulation_first_state.xml\"  # your initial state file\n",
    "LOG_DIR= \"logs\"\n",
    "\n",
    "# Global timing constants\n",
    "REDUCE_AVG_WAIT_TIME_W, FAIRNESS_W = 0.5, 0.5 # prioritize reduce wait time\n",
    "CYCLE_LENGTH_DEFAULT = 90   # s\n",
    "MAX_CYCLE_LENGTH = 120      # s\n",
    "MIN_CYCLE_LENGTH = 60      # s\n",
    "LOST_TIME = 12              # s (amber + all-red total)\n",
    "GREEN_MIN = 15             # s per direction\n",
    "MIN_GREEN_SPLIT = GREEN_MIN / (MIN_CYCLE_LENGTH-LOST_TIME)  # minimum green split ratio\n",
    "END_TIME = 16200\n",
    "CHECK_POINT_INTERVAL = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "21d6af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intersection metadata\n",
    "# Define lane groups for metric aggregation\n",
    "NS_LANES = [\"1200728225#1_0\", \"1200728225#1_1\", \"1221994726#0_0\", \"1221994726#0_1\"]  # replace with your real lane IDs\n",
    "EW_LANES = [\"1265822568#3_0\", \"1265822568#3_1\", \"1265822568#3_2\"]\n",
    "\n",
    "# Your target traffic light ID\n",
    "TL_ID = \"cluster_13075564400_13075589603_411926344\"  # replace with your actual traffic light id in SUMO net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c77475",
   "metadata": {},
   "source": [
    "## Simulation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "06c650b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_sumo_with_state(state_path):\n",
    "    \"\"\"\n",
    "    Reload the current SUMO simulation from a saved state safely.\n",
    "    (Wrapper around traci.load)\n",
    "    Args:\n",
    "        sumo_cfg (str): Path to the SUMO configuration file (.sumocfg).\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        state_path (str): Path to the saved simulation state (.xml or .xml.gz).\n",
    "        begin_time (float): Simulation time to resume from (matches saveState time).\n",
    "    \"\"\"\n",
    "    # --- Parse snapshot time from XML header ---\n",
    "    try:\n",
    "        with open(state_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if 'snapshot' in line and 'time=' in line:\n",
    "                    # Extract value like time=\"2791.00\"\n",
    "                    time_str = line.split('time=\"')[1].split('\"')[0]\n",
    "                    begin_time = float(time_str)\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"No <snapshot> header with 'time' found in state file.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"❌ Failed to read time from state file '{state_path}': {e}\")\n",
    "    \n",
    "    cmd = [\n",
    "        \"-c\", SUMO_CFG,\n",
    "        \"--seed\", str(SEED),\n",
    "        \"--load-state\", state_path,\n",
    "        \"--begin\", str(begin_time)\n",
    "    ]\n",
    "    traci.load(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40166581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_avg_halt_range(ns_lanes, ew_lanes):\n",
    "    ns_delay_proxy = ew_delay_proxy = 0\n",
    "    max_avg_halt = 0\n",
    "    min_avg_halt = 99999\n",
    "\n",
    "    min_fairness = 1\n",
    "    max_fairness = 0\n",
    "    while traci.simulation.getTime() < END_TIME:\n",
    "        ns_delay_proxy = 0\n",
    "        ew_delay_proxy = 0\n",
    "        for _ in range(CYCLE_LENGTH_DEFAULT):\n",
    "            traci.simulationStep()\n",
    "            signal_state = traci.trafficlight.getRedYellowGreenState(TL_ID)\n",
    "\n",
    "            # Split signal state: first 5 (EW), last 7 (NS)\n",
    "            ew_state = signal_state[:5]\n",
    "            ns_state = signal_state[5:]\n",
    "\n",
    "            # Detect if direction is in red\n",
    "            ew_red = \"r\" in ew_state\n",
    "            ns_red = \"r\" in ns_state\n",
    "            # Accumulate waiting times and vehicles only when direction is red\n",
    "            if ns_red:\n",
    "                ns_delay_proxy += sum(traci.lane.getLastStepHaltingNumber(l) for l in ns_lanes)\n",
    "            if ew_red:\n",
    "                ew_delay_proxy += sum(traci.lane.getLastStepHaltingNumber(l) for l in ew_lanes)\n",
    "                \n",
    "            # Total average queue length\n",
    "            total_avg_halt = (ns_delay_proxy + ew_delay_proxy)\n",
    "             # Fairness (bounded 0–1)\n",
    "            fainess = abs(ns_delay_proxy - ew_delay_proxy) / (ns_delay_proxy + ew_delay_proxy + 1e-5)\n",
    "        if(total_avg_halt > max_avg_halt):\n",
    "            max_avg_halt = total_avg_halt\n",
    "        if(total_avg_halt < min_avg_halt):\n",
    "            min_avg_halt = total_avg_halt\n",
    "        if(fainess > max_fairness):\n",
    "            max_fairness = fainess\n",
    "        if(fainess < min_fairness):\n",
    "            min_fairness = fainess\n",
    "\n",
    "    print(f\"❌ Stopping at END_TIME.\")\n",
    "    usable_time = CYCLE_LENGTH_DEFAULT - LOST_TIME\n",
    "    \n",
    "    return max_avg_halt/usable_time,min_avg_halt/usable_time,max_fairness,min_fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "06dbec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1265822568#3_0', '1265822568#3_0', '1265822568#3_1', '1265822568#3_2', '1265822568#3_2', '1200728225#1_0', '1200728225#1_1', '1200728225#1_1', '1200728225#1_1', '1221994726#0_0', '1221994726#0_0', '1221994726#0_1')\n",
      "rrrrrGGggGGG\n",
      "❌ Stopping at END_TIME.\n",
      "Max avg halt 29.0\n",
      "Min avg halt 3.0641025641025643\n",
      "Max fairness 0.9999999824868655\n",
      "Min fairness 0.00494699643146999\n"
     ]
    }
   ],
   "source": [
    "sumoCmd = [\n",
    "    SUMO_BINARY,\n",
    "    \"-c\", SUMO_CFG,\n",
    "    \"--seed\", str(SEED),\n",
    "]\n",
    "traci.start(sumoCmd)\n",
    "print(traci.trafficlight.getControlledLanes(TL_ID))\n",
    "traci.simulationStep()\n",
    "traci.simulation.saveState(SUMO_FIRST_STATE)\n",
    "signal_state = traci.trafficlight.getRedYellowGreenState(TL_ID)\n",
    "print(signal_state)\n",
    "\n",
    "(max_avg_halt,min_avg_halt,max_fairness,min_fairness) = find_avg_halt_range(NS_LANES, EW_LANES)\n",
    "print('Max avg halt', max_avg_halt)\n",
    "print('Min avg halt', min_avg_halt)\n",
    "print('Max fairness', max_fairness)\n",
    "print('Min fairness', min_fairness)\n",
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3926d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delay rate (queue area per usable second)\n",
    "MIN_DELAY_RATE_PER_CYCLE = 0\n",
    "MAX_DELAY_RATE_PER_CYCLE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c0750722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_plan(tl_id, g_main, g_cross, amber=3, all_red=3):\n",
    "    \"\"\"\n",
    "    Define a 2-phase signal plan (NS and EW) with amber and all-red times.\n",
    "    \"\"\"\n",
    "    amrber_red_phase_duration = amber + all_red\n",
    "    phases = [\n",
    "        traci.trafficlight.Phase(g_main, \"rrrrrGGggGGG\"),   # NS green\n",
    "        traci.trafficlight.Phase(amrber_red_phase_duration, \"rrrrryyyyyyy\"),    # NS amber\n",
    "        traci.trafficlight.Phase(g_cross, \"GGGGGrrrrrrr\"),  # EW green\n",
    "        traci.trafficlight.Phase(amrber_red_phase_duration, \"yyyyyrrrrrrr\"),    # EW amber\n",
    "    ]\n",
    "\n",
    "    logic = traci.trafficlight.Logic(\"custom_logic\", 0, 0, phases)\n",
    "    traci.trafficlight.setCompleteRedYellowGreenDefinition(tl_id, logic)\n",
    "\n",
    "    # Metrics calculation\n",
    "def cycle_metrics(ns_lanes, ew_lanes, steps):\n",
    "    ns_delay_proxy = ew_delay_proxy = 0\n",
    "\n",
    "    for _ in range(steps):\n",
    "        traci.simulationStep()\n",
    "        signal_state = traci.trafficlight.getRedYellowGreenState(TL_ID)\n",
    "\n",
    "         # Split signal state: first 5 (EW), last 7 (NS)\n",
    "        ew_state = signal_state[:5]\n",
    "        ns_state = signal_state[5:]\n",
    "\n",
    "        # Detect if direction is in red\n",
    "        ew_red = \"r\" in ew_state\n",
    "        ns_red = \"r\" in ns_state\n",
    "        # Accumulate waiting times and vehicles only when direction is red\n",
    "        if ns_red:\n",
    "            ns_delay_proxy += sum(traci.lane.getLastStepHaltingNumber(l) for l in ns_lanes)\n",
    "        if ew_red:\n",
    "            ew_delay_proxy += sum(traci.lane.getLastStepHaltingNumber(l) for l in ew_lanes)\n",
    "            \n",
    "    # Fairness (bounded 0–1)\n",
    "    O2_norm = abs(ns_delay_proxy - ew_delay_proxy) / (ns_delay_proxy + ew_delay_proxy + 1e-5)\n",
    "    # Total average queue length\n",
    "    delay_rate = (ns_delay_proxy + ew_delay_proxy)/(steps-LOST_TIME)\n",
    "    O1_norm = (delay_rate - MIN_DELAY_RATE_PER_CYCLE) / (MAX_DELAY_RATE_PER_CYCLE - MIN_DELAY_RATE_PER_CYCLE)\n",
    "\n",
    "    return O1_norm, O2_norm\n",
    "\n",
    "def get_green_split(s, C):\n",
    "    g_main = max(GREEN_MIN, round((C - LOST_TIME) * s))\n",
    "    g_cross = max(GREEN_MIN, (C - LOST_TIME) - g_main)\n",
    "    return g_main, g_cross\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2913e",
   "metadata": {},
   "source": [
    "## Evaluate Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6dc8457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(s, C, isReset=True):\n",
    "    try:    \n",
    "        # Restore identical starting conditions before evaluating this candidate\n",
    "        if isReset:\n",
    "            reload_sumo_with_state(SUMO_STATE)\n",
    "        g_main, g_cross = get_green_split(s, C)\n",
    "        apply_plan(TL_ID, g_main, g_cross)\n",
    "        steps = int(C)\n",
    "        O1_norm, O2_norm = cycle_metrics(NS_LANES, EW_LANES, steps)\n",
    "\n",
    "    except traci.exceptions.FatalTraCIError as e:\n",
    "        print(\"⚠️ SUMO crashed during evaluation:\", e)\n",
    "        return C, 1  # return a default\n",
    "    return O1_norm, O2_norm\n",
    "\n",
    "def score_function(O1, O2):\n",
    "    score = (REDUCE_AVG_WAIT_TIME_W * O1) + (FAIRNESS_W * O2)\n",
    "    return score\n",
    "\n",
    "# x consist of split-Cycle\n",
    "def evaluate_candidate(x):\n",
    "    s, C = float(x[0]), round(float(x[1]))  # round C for realism\n",
    "    O1, O2 = evaluate(s, C)\n",
    "    return O1, O2\n",
    "\n",
    "def robust_evaluate(x, time_passed, n_cycles=2, λ=0.2):\n",
    "    \"\"\"Returns (robust_score, mean_O1, mean_O2)\"\"\"\n",
    "    s, C = float(x[0]), round(float(x[1]))\n",
    "    # load previous state before evaluate robust\n",
    "    reload_sumo_with_state(SUMO_STATE)\n",
    "    scores = []\n",
    "    O1s = []\n",
    "    O2s = []\n",
    "    for _ in range(n_cycles):\n",
    "        O1, O2 = evaluate(s, C, isReset=False)  # continue simulation\n",
    "        O1s.append(O1)\n",
    "        O2s.append(O2)\n",
    "        scores.append(score_function(O1, O2))\n",
    "    robust_score = np.mean(scores) + λ * np.std(scores)\n",
    "    # return O1, O2 of first evaluation\n",
    "    return robust_score, O1s[0], O2s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "333b80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DEResult:\n",
    "    s: float\n",
    "    C: float\n",
    "    O1: float\n",
    "    O2: float\n",
    "    score: float\n",
    "    elapsed: float\n",
    "    gen_history: list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af7ae1f",
   "metadata": {},
   "source": [
    "## Evolution Algorithm Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "10daeac2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Include previous elite as first individual (temporal continuity)\n",
    "def init_population(bounds, pop_size, elite_last=None):\n",
    "    pop = [np.array([random.uniform(*b) for b in bounds]) for _ in range(pop_size)]\n",
    "    if elite_last is not None:\n",
    "        pop[0] = np.array([elite_last[0], elite_last[1]])  # temporal continuity\n",
    "    return pop\n",
    "\n",
    "# Early stopping check using std\n",
    "def early_stop_check(gen_history, patience, min_delta, elapsed, time_budget_s):\n",
    "    if len(gen_history) > patience:\n",
    "        recent_scores = [h[-1] for h in gen_history[-patience:]]\n",
    "        score_std = np.std(recent_scores)\n",
    "        if score_std < min_delta:\n",
    "            print(f\"Early stopping: score std {score_std:.6f} < {min_delta}\")\n",
    "            return True\n",
    "        \n",
    "    if elapsed >= time_budget_s:\n",
    "            print(f\"Early stopping: time lapsed {elapsed}s\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    " # --- Select top-K and re-evaluate robustly\n",
    "def final_robust_selection(pop, scores, K_ratio=0.1):\n",
    "    K = round(len(pop) * K_ratio)\n",
    "    ranked = sorted(\n",
    "        [(pop[i], scores[i], score_function(*scores[i])) for i in range(len(pop))],\n",
    "        key=lambda x: x[2]\n",
    "    )\n",
    "    topK = ranked[:K]\n",
    "    evaluated = [(x, *robust_evaluate(x)) for x, _, _ in topK] #(x, robust_score, mean_O1, mean_O2)\n",
    "    winner, best_robust_score, O1_first, O2_first = min(evaluated, key=lambda z: z[1])\n",
    "    s_best, C_best = winner\n",
    "    return s_best, C_best, O1_first, O2_first, best_robust_score\n",
    "    \n",
    "def reflect_in_bounds(x, lo, hi):\n",
    "    if x < lo: return lo + (lo - x)\n",
    "    if x > hi: return hi - (x - hi)\n",
    "    return x\n",
    "    \n",
    "def evolve_generation(pop, scores, bounds, F, CR):\n",
    "    scale = np.array([1.0, 1.0 / (bounds[1][1] - bounds[1][0])])  # scale by range\n",
    "    F_vec = F*scale\n",
    "    pop_size = len(pop)\n",
    "    new_pop, new_scores = pop.copy(), scores.copy()\n",
    "    dimensions = len(bounds); lower_bound, upper_bound = np.asarray(bounds).T\n",
    "\n",
    "    for i in range(pop_size):\n",
    "        idxs = [idx for idx in range(pop_size) if idx != i]\n",
    "        choice_idx = np.random.choice(idxs, 3, replace=False)\n",
    "        r1, r2, r3 = pop[choice_idx[0]], pop[choice_idx[1]], pop[choice_idx[2]]\n",
    "\n",
    "         # --- Mutation ---\n",
    "        mutant = r1 +  F_vec * (r2 - r3)\n",
    "        # --- Reflection bounds ---\n",
    "        mutant[0] = reflect_in_bounds(mutant[0], lower_bound[0], upper_bound[0])\n",
    "        mutant[1] = reflect_in_bounds(mutant[1], lower_bound[1], upper_bound[1])\n",
    "        \n",
    "        # Crossover (ensure at least one mutant gene)\n",
    "        cross_points = np.random.rand(dimensions) < CR\n",
    "        if not np.any(cross_points):\n",
    "            cross_points[np.random.randint(0, dimensions)] = True\n",
    "        trial = np.where(cross_points, mutant, pop[i])    \n",
    "\n",
    "        # Evaluate and selection\n",
    "        O1_t, O2_t = evaluate_candidate(trial)\n",
    "        O1_i, O2_i = scores[i]\n",
    "        score_t = score_function(O1_t, O2_t)\n",
    "        score_i = score_function(O1_i, O2_i)\n",
    "\n",
    "        if score_t < score_i:\n",
    "            new_pop[i], new_scores[i] = trial, (O1_t, O2_t)\n",
    "\n",
    "    return new_pop, new_scores\n",
    "\n",
    "def differential_evolution(\n",
    "    elite_last=None,\n",
    "    time_budget_s=60,\n",
    "    pop_size=12,\n",
    "    F=0.5,\n",
    "    CR=0.8,\n",
    "    patience=5,\n",
    "    min_delta=1e-4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Differential Evolution (DE/rand/1/bin)\n",
    "    for 2 parameters: (s, C)\n",
    "    Optimizes two objectives (O1, O2) within a strict time budget.\n",
    "\n",
    "    Returns: DEResult(s_best, C_best, O1_best, O2_best, score_best, elapsed)\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    bounds = [(MIN_GREEN_SPLIT, 1-MIN_GREEN_SPLIT), (MIN_CYCLE_LENGTH, MAX_CYCLE_LENGTH)]  # (s_min, s_max), (C_min, C_max)\n",
    "\n",
    "    pop = init_population(bounds, pop_size, elite_last)\n",
    "    scores = [None] * pop_size\n",
    "\n",
    "    # --- Evaluate initial population\n",
    "    for i in range(pop_size):\n",
    "        O1, O2 = evaluate_candidate(pop[i])\n",
    "        scores[i] = (O1, O2)\n",
    "\n",
    "    gen = 0\n",
    "    gen_history = []  # store per-generation bests\n",
    "    # --- DE optimization loop (time-bounded)\n",
    "    while time.perf_counter() - start < time_budget_s:\n",
    "        gen += 1\n",
    "        pop, scores = evolve_generation(pop, scores, bounds, F, CR)\n",
    "\n",
    "        # Summary\n",
    "        best_idx = min(range(pop_size),\n",
    "                       key=lambda k: score_function(scores[k][0], scores[k][1]))\n",
    "        best = pop[best_idx]\n",
    "        s_best, C_best = best\n",
    "        O1_best, O2_best = scores[best_idx]\n",
    "        score_best = score_function(O1_best, O2_best)\n",
    "        elapsed = time.perf_counter() - start\n",
    "\n",
    "        # print(f\"gen {gen:02d} | t={elapsed:4.1f}s | \"\n",
    "        #       f\"s={s_best:.3f} C={C_best:.1f} \"\n",
    "        #       f\"O1={O1_best:.3f} O2={O2_best:.3f} score={score_best:.3f}\")\n",
    "\n",
    "        gen_history.append((elapsed, s_best, C_best, O1_best, O2_best, score_best))\n",
    "\n",
    "        if early_stop_check(gen_history, patience, min_delta, elapsed, time_budget_s):\n",
    "            break\n",
    "\n",
    "    #TODO: skip multi cycle evaluation\n",
    "    # --- Final robust evaluation\n",
    "    # s_final, C_final, O1_first_eval, O2_first_eval, best_robust_score = final_robust_selection(pop, scores)\n",
    "    \n",
    "    # # Re-evaluate the winner once for single-cycle score\n",
    "    # score_final = score_function(O1_first_eval, O2_first_eval)\n",
    "\n",
    "    # elapsed = time.perf_counter() - start\n",
    "\n",
    "    # return DEResult(s_final, C_final, O1_first_eval, O2_first_eval, score_final, best_robust_score, elapsed, gen_history)\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    return DEResult(s_best, C_best, O1_best, O2_best, score_best, elapsed, gen_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7dd1edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_cycle_result(\n",
    "    cycle,\n",
    "    s,\n",
    "    C,\n",
    "    O1,\n",
    "    O2,\n",
    "    score_best,\n",
    "    elapsed_s,\n",
    "    suffix=\"\",\n",
    "    out_dir=LOG_DIR,\n",
    "    prefix=\"DE_cycle\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Append one cycle results to a CSV log file\n",
    "    \"\"\"\n",
    "    # --- Create output folder\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Main summary file (aggregated cycle-level results)\n",
    "    summary_file = os.path.join(out_dir, f\"{prefix}_summary_{suffix}.csv\")\n",
    "    write_header = not os.path.exists(summary_file)\n",
    "\n",
    "    with open(summary_file, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow([\n",
    "                \"cycle\", \"s\", \"C\", \"O1\", \"O2\",\n",
    "                \"score_best\", \"elapsed_s\"\n",
    "            ])\n",
    "        writer.writerow([\n",
    "            cycle, round(s, 3), round(C, 1), round(O1, 3), round(O2, 3),\n",
    "            round(score_best, 4), round(elapsed_s, 2)\n",
    "        ])\n",
    "\n",
    "    return summary_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_de_simulation(total_cycles=300):\n",
    "    # Start SUMO\n",
    "    sumoCmd = [\n",
    "        SUMO_BINARY,\n",
    "        \"-c\", SUMO_CFG,\n",
    "        \"--seed\", str(SEED),\n",
    "    ]\n",
    "    log_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = \"\"\n",
    "\n",
    "    elite = [0.5, CYCLE_LENGTH_DEFAULT]  # initial elite\n",
    "\n",
    "    traci.start(sumoCmd)\n",
    "    for cycle in range(total_cycles):\n",
    "        # load default state \n",
    "        if(cycle == 0):\n",
    "            traci.simulation.loadState(SUMO_FIRST_STATE)\n",
    "        time_pased = traci.simulation.getTime()\n",
    "        print(f\"\\nTime passed at cycle {cycle+1}: {time_pased}s.\")\n",
    "        # break on end\n",
    "        if time_pased >= END_TIME:\n",
    "            print(f\"❌ Elapsed time exceed at: cycle {cycle+1}, stopping early.\")\n",
    "            break\n",
    "        print(f\"=== Cycle {cycle+1} (t={time_pased:.1f}s) ===\")\n",
    "\n",
    "        #  Save the current live SUMO state once\n",
    "        traci.simulation.saveState(SUMO_STATE)\n",
    "\n",
    "        #  Run DE using that snapshot as baseline\n",
    "        result = differential_evolution(elite_last=elite)\n",
    "        s, C, O1, O2, score_best, elapsed_s, gen_history = (\n",
    "            result.s, result.C, result.O1, result.O2,\n",
    "            result.score, result.elapsed, result.gen_history\n",
    "        )\n",
    "        elite = (s, C)\n",
    "\n",
    "        #  Restore pre-optimization state before applying best plan\n",
    "        reload_sumo_with_state(SUMO_STATE)\n",
    "        print(\"AFTER Vehicles currently in simulation:\", traci.vehicle.getIDCount())\n",
    "          \n",
    "        #  Apply only the *best plan* to the live SUMO world\n",
    "        g_main, g_cross = get_green_split(s, C)\n",
    "        apply_plan(TL_ID, g_main, g_cross)\n",
    "\n",
    "        # Evaluate performance under fixed plan\n",
    "        O1, O2 = cycle_metrics(NS_LANES, EW_LANES, int(C))\n",
    "        score_best = score_function(O1, O2)\n",
    "            \n",
    "        #  Log and write results\n",
    "        print(\n",
    "            f\"Chosen split={s:.2f}, C={C:.1f}, \"\n",
    "            f\"O1={O1:.3f}, O2={O2:.3f}, score={score_best:.3f}, time={elapsed_s:.2f}s, gen={len(gen_history)}\"\n",
    "        )\n",
    "        # Save logs\n",
    "        log_file = log_cycle_result(\n",
    "            cycle + 1, s, C, O1, O2, score_best, elapsed_s,\n",
    "            suffix=log_suffix, out_dir=\"logs\", prefix=\"traffic_DE\"\n",
    "        )\n",
    "\n",
    "    traci.close()    \n",
    "    print(f\"\\n✅ Simulation completed — results saved to {log_file}\")\n",
    "\n",
    "# Run the function\n",
    "seed_de_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def visualize_pow_results(log_dir, suffix=\"\", prefix=\"traffic_DE\"):\n",
    "    \"\"\"\n",
    "    Visualize core PoW evidence from your DE summary CSV.\n",
    "    Expects: CSV created by log_cycle_result()\n",
    "    Plots:\n",
    "        1. Cycle length evolution (adaptivity)\n",
    "        2. Objective trends (O1, O2)\n",
    "        3. Score trend (convergence)\n",
    "        4. Optimization runtime per cycle\n",
    "        5. Baseline vs. EA improvement summary (optional if baseline exists)\n",
    "    \"\"\"\n",
    "    # --- Load summary CSV\n",
    "    summary_file = os.path.join(log_dir, f\"{prefix}_summary_{suffix}.csv\")\n",
    "    if not os.path.exists(summary_file):\n",
    "        print(f\"❌ Summary file not found: {summary_file}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(summary_file)\n",
    "    print(f\"Loaded {len(df)} cycles from {summary_file}\")\n",
    "\n",
    "    # --- 1. Cycle length evolution\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(df[\"cycle\"], df[\"C\"], marker='o')\n",
    "    plt.xlabel(\"Cycle #\")\n",
    "    plt.ylabel(\"Cycle length (s)\")\n",
    "    plt.title(\"Adaptive cycle length over time\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 2. Objective trends\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(df[\"cycle\"], df[\"O1\"], label=\"Avg delay (O1)\")\n",
    "    plt.plot(df[\"cycle\"], df[\"O2\"], label=\"Fairness (O2)\")\n",
    "    plt.xlabel(\"Cycle #\")\n",
    "    plt.ylabel(\"Objective value\")\n",
    "    plt.title(\"Objective trends over cycles\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 3. Score trend\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(df[\"cycle\"], df[\"score_best\"], label=\"Score (per cycle)\", color='orange')\n",
    "    plt.xlabel(\"Cycle #\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Optimization score per cycle\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 4. Runtime feasibility\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(df[\"cycle\"], df[\"elapsed_s\"], label=\"Runtime per optimization\", color='red')\n",
    "    plt.axhline(y=15, color='gray', linestyle='--', label=\"15s real-time bound\")\n",
    "    plt.xlabel(\"Cycle #\")\n",
    "    plt.ylabel(\"Computation time (s)\")\n",
    "    plt.title(\"Optimization runtime per cycle\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 5. Summary statistics\n",
    "    print(\"\\n=== Summary statistics ===\")\n",
    "    print(df[[\"C\",\"O1\",\"O2\",\"score_best\",\"elapsed_s\"]].describe().round(3))\n",
    "    avg_runtime = df[\"elapsed_s\"].mean()\n",
    "    print(f\"\\n✅  Average runtime per DE optimization: {avg_runtime:.2f}s\")\n",
    "    print(\"✅ Visualization complete — core PoW evidence generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pow_results(log_dir=LOG_DIR, suffix=\"20251106_225623\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d143dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_simulation(fixed_s=0.5, fixed_C=90, total_cycles=300):\n",
    "    \"\"\"\n",
    "    Baseline simulation using a fixed signal plan (no optimization).\n",
    "    Uses identical SUMO setup and logging as the DE version\n",
    "    for direct comparison.\n",
    "    \"\"\"\n",
    "    sumoCmd = [\n",
    "        SUMO_BINARY,\n",
    "        \"-c\", SUMO_CFG,\n",
    "        \"--seed\", str(SEED),\n",
    "    ]\n",
    "    log_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = \"\"\n",
    "\n",
    "    print(f\"Running baseline simulation: s={fixed_s}, C={fixed_C}s for {total_cycles} cycles\")\n",
    "\n",
    "    traci.start(sumoCmd) \n",
    "    for cycle in range(total_cycles):\n",
    "        # load default state \n",
    "        if(cycle == 0):\n",
    "            traci.simulation.loadState(SUMO_FIRST_STATE)\n",
    "        time_pased = traci.simulation.getTime()\n",
    "        print(f\"\\nTime passed at cycle {cycle+1}: {time_pased}s.\")\n",
    "        if time_pased >= END_TIME:\n",
    "            print(f\"❌ No more vehicles at cycle {cycle+1}, stopping early.\")\n",
    "            break\n",
    "        print(f\"=== Cycle {cycle+1} (t={time_pased:.1f}s) ===\")\n",
    "        \n",
    "        print(\"Vehicles currently in simulation:\", traci.vehicle.getIDCount())\n",
    "        # Apply fixed plan\n",
    "        g_main, g_cross = get_green_split(fixed_s, fixed_C)\n",
    "        apply_plan(TL_ID, g_main, g_cross)\n",
    "\n",
    "        # Evaluate performance under fixed plan\n",
    "        O1, O2 = cycle_metrics(NS_LANES, EW_LANES, int(fixed_C))\n",
    "        score_best = score_function(O1, O2)\n",
    "        elapsed_s = 0.0  # not optimized, so no runtime cost\n",
    "\n",
    "        # Log\n",
    "        print(\n",
    "            f\"Fixed split={fixed_s:.2f}, C={fixed_C:.1f}, \"\n",
    "            f\"O1={O1:.3f}, O2={O2:.3f}, score={score_best:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Write results\n",
    "        log_file = log_cycle_result(\n",
    "            cycle + 1,\n",
    "            fixed_s,\n",
    "            fixed_C,\n",
    "            O1,\n",
    "            O2,\n",
    "            score_best,\n",
    "            elapsed_s,\n",
    "            suffix=log_suffix,\n",
    "            out_dir=\"logs\",\n",
    "            prefix=\"traffic_baseline\",\n",
    "        )\n",
    "    traci.close()   \n",
    "    print(f\"\\n✅ Baseline completed — results saved to {log_file}\")\n",
    "\n",
    "baseline_simulation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77529ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pow_results(log_dir=LOG_DIR, prefix=\"traffic_baseline\", suffix=\"20251107_081346\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_logs_by_time(log_dir=LOG_DIR, baseline_prefix='traffic_baseline', de_prefix ='traffic_DE', baseline_suffix = \"\", de_suffix = \"\"):\n",
    "    \"\"\"\n",
    "    Compare DE and baseline log files by plotting s, C, O1, and O2 \n",
    "    against time lapse (cycle × C).\n",
    "\n",
    "    Parameters:\n",
    "        de_path (str): Path to DE log CSV file\n",
    "        baseline_path (str): Path to baseline log CSV file\n",
    "    \"\"\"\n",
    "\n",
    "    de_path = os.path.join(log_dir, f\"{de_prefix}_summary_{de_suffix}.csv\")\n",
    "    baseline_path = os.path.join(log_dir, f\"{baseline_prefix}_summary_{baseline_suffix}.csv\")\n",
    "    if not os.path.exists(de_path):\n",
    "        print(f\"❌ Summary file not found: {de_path}\")\n",
    "        return\n",
    "    if not os.path.exists(baseline_path):\n",
    "        print(f\"❌ Summary file not found: {baseline_path}\")\n",
    "        return\n",
    "    # Load CSVs\n",
    "    de = pd.read_csv(de_path)\n",
    "    baseline = pd.read_csv(baseline_path)\n",
    "\n",
    "     # Ensure same length / alignment\n",
    "    min_len = min(len(de), len(baseline))\n",
    "    de = de.head(min_len)\n",
    "    baseline = baseline.head(min_len)\n",
    "\n",
    "     # Variables to compare\n",
    "    vars_to_plot = [\"s\", \"C\", \"O1\", \"O2\", \"score_best\"]\n",
    "\n",
    "    # Plot each variable\n",
    "    for v in vars_to_plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(de[\"cycle\"], de[v], label=\"DE\", linewidth=2)\n",
    "        plt.plot(baseline[\"cycle\"], baseline[v], label=\"Baseline\", linestyle=\"--\", linewidth=2)\n",
    "        plt.xlabel(\"Cycle\")\n",
    "        plt.ylabel(v)\n",
    "        plt.title(f\"Comparison of {v} over Cycle\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "compare_logs_by_time(de_suffix='20251106_225623', baseline_suffix='20251107_081346')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_score_best(log_dir=LOG_DIR, baseline_prefix='traffic_baseline', de_prefix ='traffic_DE', baseline_suffix = \"\", de_suffix = \"\"):\n",
    "    \"\"\"\n",
    "    Compare DE vs Baseline based on score_best over cycles.\n",
    "    Shows a line plot and prints basic improvement summary.\n",
    "    \n",
    "    Parameters:\n",
    "        de_path (str): Path to DE log CSV file\n",
    "        baseline_path (str): Path to baseline log CSV file\n",
    "    \"\"\"\n",
    "    de_path = os.path.join(log_dir, f\"{de_prefix}_summary_{de_suffix}.csv\")\n",
    "    baseline_path = os.path.join(log_dir, f\"{baseline_prefix}_summary_{baseline_suffix}.csv\")\n",
    "    if not os.path.exists(de_path):\n",
    "        print(f\"❌ Summary file not found: {de_path}\")\n",
    "        return\n",
    "    if not os.path.exists(baseline_path):\n",
    "        print(f\"❌ Summary file not found: {baseline_path}\")\n",
    "        return\n",
    "    # Load CSVs\n",
    "    de = pd.read_csv(de_path)\n",
    "    baseline = pd.read_csv(baseline_path)\n",
    "\n",
    "    # Load the CSVs\n",
    "    de = pd.read_csv(de_path)\n",
    "    baseline = pd.read_csv(baseline_path)\n",
    "\n",
    "    # Ensure same length / alignment\n",
    "    min_len = min(len(de), len(baseline))\n",
    "    de = de.head(min_len)\n",
    "    baseline = baseline.head(min_len)\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(de[\"cycle\"], de[\"score_best\"], label=\"DE\", linewidth=2)\n",
    "    plt.plot(baseline[\"cycle\"], baseline[\"score_best\"], label=\"Baseline\", linestyle=\"--\", linewidth=2)\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"score_best\")\n",
    "    plt.title(\"Comparison of score_best: DE vs Baseline\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary stats\n",
    "    mean_de = de[\"score_best\"].mean()\n",
    "    mean_base = baseline[\"score_best\"].mean()\n",
    "    diff = mean_de - mean_base\n",
    "\n",
    "    print(f\"Average DE score_best: {mean_de:.4f}\")\n",
    "    print(f\"Average Baseline score_best: {mean_base:.4f}\")\n",
    "    print(f\"Δ Improvement: {diff:+.4f} ({(diff / mean_base) * 100:.2f}% change)\")\n",
    "\n",
    "    if diff > 0:\n",
    "        print(\"✅ DE improves over Baseline.\")\n",
    "    elif diff < 0:\n",
    "        print(\"❌ DE performs worse than Baseline.\")\n",
    "    else:\n",
    "        print(\"⚖️ DE and Baseline perform equally.\")\n",
    "\n",
    "compare_score_best(de_suffix='20251107_171335', baseline_suffix='20251107_171421')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de_traffic_control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
